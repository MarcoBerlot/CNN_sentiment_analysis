{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)\n",
    "PAD_TOKEN = '_PAD_'\n",
    "UNK_TOKEN = '_UNK_'\n",
    "\n",
    "\n",
    "\n",
    "# Methods for loading SST data\n",
    "\n",
    "def sentiment2label(x):\n",
    "  if x >= 0 and x <= 0.2:\n",
    "    return 0\n",
    "  elif x > 0.2 and x <= 0.4:\n",
    "    return 1\n",
    "  elif x > 0.4 and x <= 0.6:\n",
    "    return 2\n",
    "  elif x > 0.6 and x <= 0.8:\n",
    "    return 3\n",
    "  elif x > 0.8 and x <= 1:\n",
    "    return 4\n",
    "  else:\n",
    "    raise ValueError('Improper sentiment value {}'.format(x))\n",
    "\n",
    "\n",
    "def read_dictionary_txt_with_phrase_ids(dictionary_path, phrase_ids_path, labels_path=None):\n",
    "  print('Reading data dictionary_path={} phrase_ids_path={} labels_path={}'.format(\n",
    "    dictionary_path, phrase_ids_path, labels_path))\n",
    "\n",
    "  with open(phrase_ids_path) as f:\n",
    "    phrase_ids = set(line.strip() for line in f)\n",
    "\n",
    "  with open(dictionary_path) as f:\n",
    "    examples_dict = dict()\n",
    "    for line in f:\n",
    "      parts = line.strip().split('|')\n",
    "      phrase = parts[0]\n",
    "      phrase_id = parts[1]\n",
    "\n",
    "      if phrase_id not in phrase_ids:\n",
    "        continue\n",
    "\n",
    "      example = dict()\n",
    "      example['phrase'] = phrase.replace('(', '-LRB').replace(')', '-RRB-')\n",
    "      example['tokens'] = example['phrase'].split(' ')\n",
    "      example['example_id'] = phrase_id\n",
    "      example['label'] = None\n",
    "      examples_dict[example['example_id']] = example\n",
    "\n",
    "  if labels_path is not None:\n",
    "    with open(labels_path) as f:\n",
    "      for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "          continue\n",
    "        parts = line.strip().split('|')\n",
    "        phrase_id = parts[0]\n",
    "        sentiment = float(parts[1])\n",
    "        label = sentiment2label(sentiment)\n",
    "\n",
    "        if phrase_id in examples_dict:\n",
    "          examples_dict[phrase_id]['label'] = label\n",
    "\n",
    "  examples = [ex for _, ex in examples_dict.items()]\n",
    "\n",
    "  print('Found {} examples.'.format(len(examples)))\n",
    "\n",
    "  return examples\n",
    "\n",
    "\n",
    "def build_vocab(datasets):\n",
    "  vocab = dict()\n",
    "  vocab[PAD_TOKEN] = len(vocab)\n",
    "  vocab[UNK_TOKEN] = len(vocab)\n",
    "  for data in datasets:\n",
    "    for example in data:\n",
    "      for word in example['tokens']:\n",
    "        if word not in vocab:\n",
    "          vocab[word] = len(vocab)\n",
    "\n",
    "  print('Vocab size: {}'.format(len(vocab)))\n",
    "\n",
    "  return vocab\n",
    "\n",
    "\n",
    "class TokenConverter(object):\n",
    "  def __init__(self, vocab):\n",
    "    self.vocab = vocab\n",
    "    self.unknown = 0\n",
    "\n",
    "  def convert(self, token):\n",
    "    if token in self.vocab:\n",
    "      id = self.vocab.get(token)\n",
    "    else:\n",
    "      id = self.vocab.get(UNK_TOKEN)\n",
    "      self.unknown += 1\n",
    "    return id\n",
    "\n",
    "\n",
    "def convert2ids(data, vocab):\n",
    "  converter = TokenConverter(vocab)\n",
    "  for example in data:\n",
    "    example['tokens'] = list(map(converter.convert, example['tokens']))\n",
    "  print('Found {} unknown tokens.'.format(converter.unknown))\n",
    "  return data\n",
    "\n",
    "\n",
    "def load_data_and_embeddings(data_path, phrase_ids_path, embeddings_path):\n",
    "  labels_path = os.path.join(data_path, 'sentiment_labels.txt')\n",
    "  dictionary_path = os.path.join(data_path, 'dictionary.txt')\n",
    "  train_data = read_dictionary_txt_with_phrase_ids(dictionary_path, os.path.join(phrase_ids_path, 'phrase_ids.train.txt'), labels_path)\n",
    "  validation_data = read_dictionary_txt_with_phrase_ids(dictionary_path, os.path.join(phrase_ids_path, 'phrase_ids.dev.txt'), labels_path)\n",
    "  test_data = read_dictionary_txt_with_phrase_ids(dictionary_path, os.path.join(phrase_ids_path, 'phrase_ids.test.txt'))\n",
    "  vocab = build_vocab([train_data, validation_data, test_data])\n",
    "  vocab, embeddings = load_embeddings('./glove.840B.300d.txt', vocab, cache=True)\n",
    "  train_data = convert2ids(train_data, vocab)\n",
    "  validation_data = convert2ids(validation_data, vocab)\n",
    "  test_data = convert2ids(test_data, vocab)\n",
    "  return train_data, validation_data, test_data, vocab, embeddings\n",
    "\n",
    "\n",
    "def load_embeddings(path, vocab, cache=False, cache_path=None):\n",
    "  rows = []\n",
    "  new_vocab = [UNK_TOKEN]\n",
    "\n",
    "  if cache_path is None:\n",
    "    cache_path = path + '.cache'\n",
    "\n",
    "  # Use cache file if it exists.\n",
    "  if os.path.exists(cache_path):\n",
    "    path = cache_path\n",
    "\n",
    "  print(\"Reading embeddings from {}\".format(path))\n",
    "\n",
    "  # first pass over the embeddings to vocab and relevant rows\n",
    "  with open(path) as f:\n",
    "    for line in f:\n",
    "      word, row = line.split(' ', 1)\n",
    "      if word == UNK_TOKEN:\n",
    "        raise ValueError('The unk token should not exist w.in embeddings.')\n",
    "      if word in vocab:\n",
    "        rows.append(line)\n",
    "        new_vocab.append(word)\n",
    "\n",
    "  # optionally save relevant rows to cache file.\n",
    "  if cache and not os.path.exists(cache_path):\n",
    "    with open(cache_path, 'w') as f:\n",
    "      for line in rows:\n",
    "        f.write(line)\n",
    "      print(\"Cached embeddings to {}\".format(cache_path))\n",
    "\n",
    "  # turn vocab list into a dictionary\n",
    "  new_vocab = {w: i for i, w in enumerate(new_vocab)}\n",
    "\n",
    "  print('New vocab size: {}'.format(len(new_vocab)))\n",
    "\n",
    "  assert len(rows) == len(new_vocab) - 1\n",
    "\n",
    "  # create embeddings matrix\n",
    "  embeddings = np.zeros((len(new_vocab), 300), dtype=np.float32)\n",
    "  for i, line in enumerate(rows):\n",
    "    embeddings[i+1] = list(map(float, line.strip().split(' ')[1:]))\n",
    "\n",
    "  return new_vocab, embeddings\n",
    "\n",
    "\n",
    "# Batch Iterator\n",
    "\n",
    "def prepare_data(data):\n",
    "  # pad data\n",
    "  maxlen = max(map(len, data))\n",
    "  data = [ex + [0] * (maxlen-len(ex)) for ex in data]\n",
    "\n",
    "  # wrap in tensor\n",
    "  return torch.LongTensor(data)\n",
    "\n",
    "\n",
    "def prepare_labels(labels):\n",
    "  try:\n",
    "    return torch.LongTensor(labels)\n",
    "  except:\n",
    "    return labels\n",
    "\n",
    "\n",
    "def batch_iterator(dataset, batch_size, forever=False):\n",
    "  dataset_size = len(dataset)\n",
    "  order = None\n",
    "  nbatches = dataset_size // batch_size\n",
    "\n",
    "  def init_order():\n",
    "    return random.sample(range(dataset_size), dataset_size)\n",
    "\n",
    "  def get_batch(start, end):\n",
    "    batch = [dataset[ii] for ii in order[start:end]]\n",
    "    data = prepare_data([ex['tokens'] for ex in batch])\n",
    "    labels = prepare_labels([ex['label'] for ex in batch])\n",
    "    example_ids = [ex['example_id'] for ex in batch]\n",
    "    return data, labels, example_ids\n",
    "\n",
    "  order = init_order()\n",
    "\n",
    "  while True:\n",
    "    for i in range(nbatches):\n",
    "      start = i*batch_size\n",
    "      end = (i+1)*batch_size\n",
    "      yield get_batch(start, end)\n",
    "\n",
    "    if nbatches*batch_size < dataset_size:\n",
    "      yield get_batch(start, end)\n",
    "\n",
    "    if not forever:\n",
    "      break\n",
    "    \n",
    "    order = init_order()\n",
    "\n",
    "\n",
    "# Models\n",
    "\n",
    "class BagOfWordsModel(nn.Module):\n",
    "  def __init__(self, embeddings):\n",
    "    super(BagOfWordsModel, self).__init__()\n",
    "    self.embed = nn.Embedding(embeddings.shape[0], embeddings.shape[1], sparse=True)\n",
    "    self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "    self.classify = nn.Linear(embeddings.shape[1], 5)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.classify(self.embed(x).sum(1))\n",
    "\n",
    "\n",
    "# Utility Methods\n",
    "\n",
    "def checkpoint_model(step, val_err, model, opt, save_path):\n",
    "  save_dict = dict(\n",
    "    step=step,\n",
    "    val_err=val_err,\n",
    "    model_state_dict=model.state_dict(),\n",
    "    opt_state_dict=opt.state_dict())\n",
    "  torch.save(save_dict, save_path)\n",
    "\n",
    "\n",
    "def load_model(model, opt, load_path):\n",
    "  load_dict = torch.load(load_path)\n",
    "  step = load_dict['step']\n",
    "  val_err = load_dict['val_err']\n",
    "  model.load_state_dict(load_dict['model_state_dict'])\n",
    "  opt.load_state_dict(load_dict['opt_state_dict'])\n",
    "  return step, val_err\n",
    "    \n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return Variable(LongTensor([word2index[word]]) if word2index.get(word) is not None else LongTensor([word2index[\"<UNK>\"]]))\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "gpus = [0]\n",
    "torch.cuda.set_device(gpus[0])\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data dictionary_path=./stanfordSentimentTreebank\\dictionary.txt phrase_ids_path=./phrase_ids.train.txt labels_path=./stanfordSentimentTreebank\\sentiment_labels.txt\n",
      "Found 159274 examples.\n",
      "Reading data dictionary_path=./stanfordSentimentTreebank\\dictionary.txt phrase_ids_path=./phrase_ids.dev.txt labels_path=./stanfordSentimentTreebank\\sentiment_labels.txt\n",
      "Found 24772 examples.\n",
      "Reading data dictionary_path=./stanfordSentimentTreebank\\dictionary.txt phrase_ids_path=./phrase_ids.test.txt labels_path=None\n",
      "Found 46663 examples.\n",
      "Vocab size: 21703\n",
      "Reading embeddings from ./glove.840B.300d.txt.cache\n",
      "New vocab size: 18640\n",
      "Found 8758 unknown tokens.\n",
      "Found 1255 unknown tokens.\n",
      "Found 21205 unknown tokens.\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data, vocab, embeddings = \\\n",
    "load_data_and_embeddings(\"./stanfordSentimentTreebank\", \"./\", './glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[[d['phrase'],d['label']] for d in train_data]\n",
    "validation=[[d['phrase'],d['label']] for d in validation_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = list(zip(*data))\n",
    "X = list(X)\n",
    "X_val, y_val=list(zip(*validation))\n",
    "X_val=list(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(X):\n",
    "    X[i] = re.sub('\\d', '#', str(x)).split()\n",
    "    \n",
    "for i, x in enumerate(X_val):\n",
    "    X_val[i] = re.sub('\\d', '#', str(x)).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={'<PAD>': 0, '<UNK>': 1}\n",
    "\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "        \n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "target2index = {}\n",
    "\n",
    "for cl in set(y):\n",
    "    if target2index.get(cl) is None:\n",
    "        target2index[cl] = len(target2index)\n",
    "\n",
    "index2target = {v:k for k, v in target2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p, y_p = [], []\n",
    "X_val_p=[]\n",
    "y_val_p=[]\n",
    "\n",
    "for pair in zip(X,y):\n",
    "    X_p.append(prepare_sequence(pair[0], word2index).view(1, -1))\n",
    "    y_p.append(Variable(LongTensor([target2index[pair[1]]])).view(1, -1))\n",
    "for pair in zip(X_val,y_val):\n",
    "    X_val_p.append(prepare_sequence(pair[0], word2index).view(1, -1))\n",
    "    y_val_p.append(Variable(LongTensor([target2index[pair[1]]])).view(1, -1))\n",
    "\n",
    "data_p = list(zip(X_p, y_p))\n",
    "\n",
    "train_data = data_p[: int(len(data_p) * 0.9)]\n",
    "test_data = data_p[int(len(data_p) * 0.9):]\n",
    "val_data=list(zip(X_val_p, y_val_p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mb2589\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(name):\n",
    "    if(name==\"word2vec\"):\n",
    "        print (\"Loading Word2vec\")\n",
    "        import gensim\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    if(name==\"glove\"):\n",
    "        import numpy as np\n",
    "        print (\"Loading Glove Model\")\n",
    "        f = open(\"./glove.840B.300d.txt.cache\",'r')\n",
    "        model = {}\n",
    "        for line in f:\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            model[word] = embedding\n",
    "        print (\"Done.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model=loadModel(\"glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = []\n",
    "\n",
    "for key in index2word.keys():\n",
    "    try:\n",
    "        pretrained.append(model[index2word[key]])\n",
    "        \n",
    "    except:\n",
    "        pretrained.append(np.random.randn(300))\n",
    "        \n",
    "pretrained_vectors = np.vstack(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = []\n",
    "\n",
    "for key in index2word.keys():\n",
    "    try:\n",
    "        pretrained.append(glove[index2word[key]])\n",
    "        \n",
    "    except:\n",
    "        pretrained.append(np.random.randn(300))\n",
    "        \n",
    "pretrained_vectors = np.vstack(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  CNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, kernel_dim=100, kernel_sizes=(3, 4, 5), dropout=0.5):\n",
    "        super(CNNClassifier,self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_dim, (K, embedding_dim)) for K in kernel_sizes])\n",
    "\n",
    "        # kernal_size = (K,D) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * kernel_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def init_weights(self, pretrained_word_vectors, is_static=False):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "        if is_static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, inputs, is_training=False):\n",
    "        inputs = self.embedding(inputs).unsqueeze(1) # (B,1,T,D)\n",
    "        inputs = [F.relu(conv(inputs)).squeeze(3) for conv in self.convs] #[(N,Co,W), ...]*len(Ks)\n",
    "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] #[(N,Co), ...]*len(Ks)\n",
    "\n",
    "        concated = torch.cat(inputs, 1)\n",
    "\n",
    "        if is_training:\n",
    "            concated = self.dropout(concated) # (N,len(Ks)*Co)\n",
    "        out = self.fc(concated) \n",
    "        return F.log_softmax(out,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 50\n",
    "KERNEL_SIZES = [1]\n",
    "KERNEL_DIM = 100\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNClassifier(len(word2index), 300, len(target2index), KERNEL_DIM, KERNEL_SIZES)\n",
    "#model.init_weights(pretrained_vectors) # initialize embedding matrix using pretrained vectors\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch\n",
    "        \n",
    "def pad_to_batch(batch):\n",
    "    x,y = zip(*batch)\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    x_p = []\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1) < max_x:\n",
    "            x_p.append(torch.cat([x[i], Variable(LongTensor([word2index['<PAD>']] * (max_x - x[i].size(1)))).view(1, -1)], 1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "    return torch.cat(x_p), torch.cat(y).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5] mean_loss : 1.48\n",
      "[0/5] mean_loss : 1.50\n",
      "[0/5] mean_loss : 1.51\n",
      "[0/5] mean_loss : 1.32\n",
      "[0/5] mean_loss : 1.20\n",
      "[0/5] mean_loss : 1.31\n",
      "[0/5] mean_loss : 1.13\n",
      "[0/5] mean_loss : 1.29\n",
      "[0/5] mean_loss : 1.28\n",
      "[0/5] mean_loss : 1.28\n",
      "[0/5] mean_loss : 1.19\n",
      "[0/5] mean_loss : 1.21\n",
      "[0/5] mean_loss : 1.19\n",
      "[0/5] mean_loss : 1.14\n",
      "[0/5] mean_loss : 1.22\n",
      "[0/5] mean_loss : 1.21\n",
      "[0/5] mean_loss : 1.11\n",
      "[0/5] mean_loss : 1.17\n",
      "[0/5] mean_loss : 1.09\n",
      "[0/5] mean_loss : 1.36\n",
      "[0/5] mean_loss : 1.14\n",
      "[0/5] mean_loss : 1.17\n",
      "[0/5] mean_loss : 1.01\n",
      "[0/5] mean_loss : 0.97\n",
      "[0/5] mean_loss : 1.06\n",
      "[0/5] mean_loss : 1.09\n",
      "[0/5] mean_loss : 1.09\n",
      "[0/5] mean_loss : 1.06\n",
      "[0/5] mean_loss : 1.00\n",
      "[1/5] mean_loss : 0.58\n",
      "[1/5] mean_loss : 1.18\n",
      "[1/5] mean_loss : 1.18\n",
      "[1/5] mean_loss : 1.04\n",
      "[1/5] mean_loss : 1.05\n",
      "[1/5] mean_loss : 1.14\n",
      "[1/5] mean_loss : 0.98\n",
      "[1/5] mean_loss : 1.11\n",
      "[1/5] mean_loss : 1.12\n",
      "[1/5] mean_loss : 1.05\n",
      "[1/5] mean_loss : 0.99\n",
      "[1/5] mean_loss : 0.99\n",
      "[1/5] mean_loss : 1.00\n",
      "[1/5] mean_loss : 0.97\n",
      "[1/5] mean_loss : 1.05\n",
      "[1/5] mean_loss : 1.02\n",
      "[1/5] mean_loss : 0.94\n",
      "[1/5] mean_loss : 1.01\n",
      "[1/5] mean_loss : 0.92\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b70a4ffc1313>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_to_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-a4c2dfa06692>\u001b[0m in \u001b[0;36mpad_to_batch\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_x\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mx_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<PAD>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_x\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mx_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses = []\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        inputs,targets = pad_to_batch(batch)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        preds = model(inputs, True)\n",
    "        \n",
    "        loss = loss_function(preds, targets)\n",
    "        losses.append(loss.data.tolist()[0])\n",
    "        loss.backward()\n",
    "        \n",
    "        #for param in model.parameters():\n",
    "        #    param.grad.data.clamp_(-3, 3)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, np.mean(losses)))\n",
    "            losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_data):\n",
    "    accuracy=0\n",
    "    for test in test_data:\n",
    "        pred = model(test[0]).max(1)[1]\n",
    "        pred = pred.data.tolist()[0]\n",
    "        target = test[1].data.tolist()[0][0]\n",
    "        if pred == target:\n",
    "            accuracy += 1\n",
    "\n",
    "    print(accuracy/len(test_data) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_CNN(model,testing_data):\n",
    "    inputs,targets = pad_to_batch(testing_data)\n",
    "    outputs = model(inputs)\n",
    "    accuracy = 0\n",
    "    for i, test_val in enumerate(testing_data):\n",
    "        num, pred = torch.max(outputs[i],0)\n",
    "        pred = pred.data.tolist()[0]\n",
    "        target = test_val[1].data.tolist()[0][0]\n",
    "        if pred == target:\n",
    "            accuracy += 1\n",
    "\n",
    "    print(accuracy/len(testing_data) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model,val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type CNNClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model,'CNN_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_stored=torch.load('CNN_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.6991461577097\n"
     ]
    }
   ],
   "source": [
    "test_CNN(model,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
